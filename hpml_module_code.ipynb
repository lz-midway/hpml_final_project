{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30628ef3-a8a1-4297-88d3-5867c8142466",
   "metadata": {},
   "source": [
    "## HPML final project: Binary Neural Network analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5ffa65-7c25-454e-9b63-844146ef8be8",
   "metadata": {},
   "source": [
    "BNCVL code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f303434-ad8f-41c4-8d37-5bfae1da6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Callable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d677f22-f5b3-434b-9693-940ea31bd1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNCVL(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, activation: Callable = nn.ReLU()):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        # 32-bit float convolution weights and bias\n",
    "        self.weight = nn.Parameter(\n",
    "            torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "        )\n",
    "        self.bias = nn.Parameter(torch.zeros(out_channels))\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def no_grad(self, x):\n",
    "        \"\"\"Stops gradient flow (Straight-Through Estimator).\"\"\"\n",
    "        return x.detach()\n",
    "    \n",
    "    def quantize(self, x):\n",
    "        \"\"\"\n",
    "        Binary quantization based on the mean of x.\n",
    "        1 if x > mean(x), else 0.\n",
    "        \"\"\"\n",
    "        threshold = x.mean()\n",
    "        return (x > threshold).float()\n",
    "    \n",
    "    def normalize(self, z):\n",
    "        \"\"\"Normalize each feature map in a sample.\"\"\"\n",
    "        mean = z.mean(dim=(1, 2, 3), keepdim=True)\n",
    "        std = z.std(dim=(1, 2, 3), keepdim=True) + 1e-8\n",
    "        return (z - mean) / std\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "            # Quantization during training (STE approximation)\n",
    "            w_q = self.weight + self.no_grad(self.quantize(self.weight) - self.weight)\n",
    "            b_q = self.bias + self.no_grad(self.quantize(self.bias) - self.bias)\n",
    "        else:\n",
    "            # Quantization during inference\n",
    "            w_q = self.quantize(self.weight)\n",
    "            b_q = self.quantize(self.bias)\n",
    "\n",
    "        # Perform convolution with quantized weights\n",
    "        z = F.conv2d(x, w_q, b_q, stride=1, padding=self.kernel_size // 2)\n",
    "\n",
    "        # Normalize and activate\n",
    "        z = self.normalize(z)\n",
    "        return self.activation(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bde077d8-1ea5-41f7-87c1-899a435590b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Output shape: torch.Size([4, 8, 32, 32])\n",
      "Unique quantized weight values: tensor([0., 1.])\n",
      "Gradient on input: 0.000014\n",
      "Gradient on weights: 0.000227\n",
      "Forward and backward pass successful.\n"
     ]
    }
   ],
   "source": [
    "# ====== TEST BLOCK ======\n",
    "\n",
    "def test_bncvl():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Create layer and move to device\n",
    "    layer = BNCVL(in_channels=3, out_channels=8, kernel_size=3).to(device)\n",
    "\n",
    "    # Create dummy input (batch=4, channels=3, height=32, width=32)\n",
    "    x = torch.randn(4, 3, 32, 32, device=device, requires_grad=True)\n",
    "\n",
    "    # Forward pass\n",
    "    y = layer(x)\n",
    "    print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "    # Check quantization (values should be 0/1 in quantized weights)\n",
    "    with torch.no_grad():\n",
    "        w_q = layer.quantize(layer.weight)\n",
    "        print(f\"Unique quantized weight values: {w_q.unique()}\")\n",
    "\n",
    "    # Backward test (gradient should flow through STE)\n",
    "    loss = y.mean()\n",
    "    loss.backward()\n",
    "\n",
    "    print(f\"Gradient on input: {x.grad.abs().mean().item():.6f}\")\n",
    "    print(f\"Gradient on weights: {layer.weight.grad.abs().mean().item():.6f}\")\n",
    "\n",
    "    assert y.shape == (4, 8, 32, 32)\n",
    "    print(\"Forward and backward pass successful.\")\n",
    "\n",
    "test_bncvl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4120c-6387-4461-9d8d-e01735bdcc91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e55725-1daf-4efa-8ae6-69dca3a6a5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hpml pytorch env",
   "language": "python",
   "name": "hpml_pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
